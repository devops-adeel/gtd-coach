"""
Minimal Langfuse integration for GTD Coach
Provides drop-in OpenAI client replacement with observability
"""

from langfuse.openai import openai
from langfuse import observe, langfuse_context
import os
import logging

# Configuration - Update these with your Langfuse instance details
LANGFUSE_HOST = "http://localhost:3000"
LANGFUSE_PUBLIC_KEY = "pk-lf-..."  # Replace with your public key
LANGFUSE_SECRET_KEY = "sk-lf-..."  # Replace with your secret key

# Set up logger
logger = logging.getLogger(__name__)

def get_langfuse_client():
    """
    Initialize and return a Langfuse-wrapped OpenAI client
    configured for LM Studio endpoint
    """
    try:
        # Set environment variables for Langfuse
        os.environ["LANGFUSE_HOST"] = LANGFUSE_HOST
        os.environ["LANGFUSE_PUBLIC_KEY"] = LANGFUSE_PUBLIC_KEY
        os.environ["LANGFUSE_SECRET_KEY"] = LANGFUSE_SECRET_KEY
        
        # Create OpenAI client with LM Studio endpoint
        client = openai.OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="lm-studio"  # LM Studio doesn't require real API key
        )
        
        logger.info("Langfuse client initialized successfully")
        return client
        
    except Exception as e:
        logger.warning(f"Failed to initialize Langfuse client: {e}")
        return None

def score_response(phase: str, success: bool, response_time: float):
    """
    Add quality scores to the current observation based on phase requirements
    
    Args:
        phase: Current GTD review phase
        success: Whether the LLM call succeeded
        response_time: Time taken for the response in seconds
    """
    try:
        # Score 1: Binary success/failure
        langfuse_context.score_current_observation(
            name="success",
            value=1 if success else 0,
            comment=f"LLM call {'succeeded' if success else 'failed'}"
        )
        
        # Score 2: Quality based on phase-specific latency thresholds
        # These thresholds are calibrated for ADHD users who need quick responses
        latency_thresholds = {
            "STARTUP": 5.0,      # Welcome phase can be slightly slower
            "MIND_SWEEP": 3.0,   # Need quick responses during capture
            "PROJECT_REVIEW": 2.0,  # Critical for 45-second per project limit
            "PRIORITIZATION": 3.0,  # Decision support needs to be responsive
            "WRAP_UP": 4.0       # Celebration phase can be more relaxed
        }
        
        threshold = latency_thresholds.get(phase, 5.0)
        quality_score = 1 if response_time < threshold else 0
        
        langfuse_context.score_current_observation(
            name="quality",
            value=quality_score,
            comment=f"Phase: {phase}, Response: {response_time:.2f}s, Threshold: {threshold}s"
        )
        
        # Score 3: Phase-specific response appropriateness
        # This helps track if the coach is meeting phase requirements
        langfuse_context.score_current_observation(
            name="phase_appropriate",
            value=1,  # Will be manually reviewed in Langfuse UI
            comment=f"Review in UI for {phase} phase appropriateness"
        )
        
    except Exception as e:
        logger.debug(f"Failed to score response: {e}")
        # Don't fail the main flow if scoring fails

def validate_configuration():
    """
    Check if Langfuse is properly configured
    
    Returns:
        bool: True if configuration appears valid
    """
    if LANGFUSE_PUBLIC_KEY == "pk-lf-..." or LANGFUSE_SECRET_KEY == "sk-lf-...":
        logger.warning("Langfuse keys not configured - please update langfuse_tracker.py")
        return False
    
    if not LANGFUSE_HOST:
        logger.warning("Langfuse host not configured")
        return False
        
    return True
